version: "3"

services:

  rclone:
    image: rclone/rclone
    volumes:
      - ./src/config:/config/rclone         # Local path to Rclone config directory
      - ./src/data/db/postgresql:/data      # Local path to the data you want to transfer
    command: config
    #command: ["copyto", "recofilm4:recofilm_db/recofilm_db_backup.sql", "recofilm_gdrive.sql", "-vv"]
    tty: true                           # Allocate a pseudo-TTY
    stdin_open: true                    # Keep stdin open
    healthcheck:
      test: ["CMD", "test", "-f", "/data/recofilm_gdrive.sql"]
      interval: 10s
      timeout: 5s
      retries: 5

  postgres-db:
    image: postgres:13
    container_name: postgres-db
    depends_on:
        rclone:
          condition: service_healthy
    #command: ["pg_restore", "-U", "admin", "-d", "recofilm_db", "-F", "c", "var/lib/postgresql/data/recofilm_gdrive.sql"]
    env_file:
      - .env
    volumes:
      - pg_project_data:/var/lib/postgresql/data
    # healthcheck:
    #   test: ["CMD", "pg_isready", "-U", "airflow"]
    #   interval: 5s
    #   retries: 5
    networks:
      - mynetwork
    ports:
      - "5433:5432"

  # app:
  #   image: chvalois/recofilm:0.1
  #   volumes: 
  #     - .:/app
  #   env_file:
  #     - .env
  #   healthcheck:
  #     test: ["CMD", "ls"]
  #     interval: 10s
  #     timeout: 10s
  #     retries: 5
  #   depends_on:
  #     - postgres-db
  #   networks:
  #     - mynetwork

  postgres-airflow:
    image: postgres:13
    container_name: postgres-airflow
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - pg_airflow_data:/var/lib/postgresql/data
    networks:
      - mynetwork
    ports:
      - "5432:5432"

  airflow-init:
    image: chvalois/recofilm:0.2
    restart: on-failure
    depends_on:
      - postgres-airflow
      - postgres-db
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: tIQHaUSQW7Ic4SbQBKoOtEm4WvuU6_go5-1QVt6lKGI=
      PYTHONPATH: /opt/airflow
    env_file:
      - .env
    volumes:
      - ./src/dags:/opt/airflow/dags
      - ./src/logs:/opt/airflow/logs
      - ./src/plugins:/opt/airflow/plugins
      - ./src:/opt/airflow/src
      - ./tests:/opt/airflow/tests
      - ./api_directory:/opt/airflow/api_directory
    entrypoint: |
      bash -c "
      airflow db init &&
      airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com
      "
    networks:
      - mynetwork

  airflow-webserver:
    image: chvalois/recofilm:0.2
    restart: always
    depends_on:
      - postgres-airflow
      - postgres-db
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: tIQHaUSQW7Ic4SbQBKoOtEm4WvuU6_go5-1QVt6lKGI=
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: 'true'
      PYTHONPATH: /opt/airflow
    env_file:
      - .env
    volumes:
      - ./src/dags:/opt/airflow/dags
      - ./src/logs:/opt/airflow/logs
      - ./src/plugins:/opt/airflow/plugins
      - ./src:/opt/airflow/src
      - ./tests:/opt/airflow/tests
      - ./api_directory:/opt/airflow/api_directory
    ports:
      - "8080:8080"
    entrypoint: /bin/bash -c "pip install pytest && airflow webserver"
    networks:
      - mynetwork

  airflow-scheduler:
    image: chvalois/recofilm:0.2
    restart: always
    depends_on:
      - postgres-airflow
      - postgres-db
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: tIQHaUSQW7Ic4SbQBKoOtEm4WvuU6_go5-1QVt6lKGI=
      PYTHONPATH: /opt/airflow
    env_file:
      - .env
    volumes:
      - ./src/dags:/opt/airflow/dags
      - ./src/logs:/opt/airflow/logs
      - ./src/plugins:/opt/airflow/plugins
      - ./src:/opt/airflow/src
      - ./tests:/opt/airflow/tests
      - ./api_directory:/opt/airflow/api_directory
    entrypoint: /bin/bash -c "pip install pytest && airflow scheduler"
    networks:
      - mynetwork
    

volumes:
  pg_airflow_data:
  pg_project_data:

networks:
  mynetwork:
    driver: bridge