version: "3"

services:

  rclone:
    image: rclone/rclone
    volumes:
      - ./src/config:/config/rclone
      - pg_project_data:/data

    command: config
    #command: ["copyto", "recofilm:recofilm_db/recofilm_gdrive_300K.sql", "db/recofilm_gdrive.sql", "-vv"]
    tty: true                           
    stdin_open: true                    
    networks:
      - mynetwork
    # healthcheck:
    #   test: ["CMD", "test", "-f", "db/recofilm_gdrive.sql"]
    #   interval: 30s
    #   timeout: 5s
    #   retries: 5
    #   start_period: 60s

  postgres-db:
    image: postgres:16
    container_name: postgres-db
    # depends_on:
    #     rclone:
    #       condition: service_healthy
    # command: ["pg_restore", "-U", "admin", "-d", "recofilm_db", "-F", "c", "var/lib/postgresql/data/recofilm_gdrive.sql"]
    env_file:
      - .env
    volumes:
      - pg_project_data:/var/lib/postgresql/data
    healthcheck:
        test: ["CMD", "pg_isready", "-U", "admin"]
        interval: 5s
        retries: 5
    networks:
      - mynetwork
    ports:
      - "5433:5432"


  postgres-airflow:
    image: postgres:16
    container_name: postgres-airflow
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - pg_airflow_data:/var/lib/postgresql/data
    networks:
      - mynetwork
    ports:
      - "5432:5432"

  airflow-init:
    image: chvalois/recofilm:0.4
    restart: on-failure
    depends_on:
      - postgres-airflow
      - postgres-db
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: tIQHaUSQW7Ic4SbQBKoOtEm4WvuU6_go5-1QVt6lKGI=
      PYTHONPATH: /opt/airflow
      MLFLOW_TRACKING_URI: http://mlflow:5001
    env_file:
      - .env
    volumes:
      - ./src/dags:/opt/airflow/dags
      - ./src/logs:/opt/airflow/logs
      - ./src/plugins:/opt/airflow/plugins
      - ./src:/opt/airflow/src
      - ./tests:/opt/airflow/tests
      - ./api_directory:/opt/airflow/api_directory
      - ./MLFlow:/opt/airflow/mlflow
    entrypoint: |
      bash -c "
      airflow db init &&
      airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com
      "
    networks:
      - mynetwork

  airflow-webserver:
    image: chvalois/recofilm:0.4
    restart: always
    depends_on:
      - postgres-airflow
      - postgres-db
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: tIQHaUSQW7Ic4SbQBKoOtEm4WvuU6_go5-1QVt6lKGI=
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: 'true'
      PYTHONPATH: /opt/airflow
      MLFLOW_TRACKING_URI: http://mlflow:5001
    env_file:
      - .env
    volumes:
      - ./src/dags:/opt/airflow/dags
      - ./src/logs:/opt/airflow/logs
      - ./src/plugins:/opt/airflow/plugins
      - ./src:/opt/airflow/src
      - ./tests:/opt/airflow/tests
      - ./api_directory:/opt/airflow/api_directory
      - ./MLFlow:/opt/airflow/mlflow
    ports:
      - "8080:8080"
    entrypoint: /bin/bash -c "pip install pytest && airflow webserver"
    networks:
      - mynetwork

  airflow-scheduler:
    image: chvalois/recofilm:0.4
    restart: always
    depends_on:
      - postgres-airflow
      - postgres-db
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: tIQHaUSQW7Ic4SbQBKoOtEm4WvuU6_go5-1QVt6lKGI=
      PYTHONPATH: /opt/airflow
      MLFLOW_TRACKING_URI: http://mlflow:5001
    env_file:
      - .env
    volumes:
      - ./src/dags:/opt/airflow/dags
      - ./src/logs:/opt/airflow/logs
      - ./src/plugins:/opt/airflow/plugins
      - ./src:/opt/airflow/src
      - ./tests:/opt/airflow/tests
      - ./api_directory:/opt/airflow/api_directory
      - ./MLFlow:/opt/airflow/mlflow
    entrypoint: /bin/bash -c "pip install pytest && airflow scheduler"
    networks:
      - mynetwork

  api:
    build:
      context: .
      dockerfile: Dockerfile_api
    image: fastapi_image:latest 
    container_name: api_docker 
    volumes:
      - ./src:/app/src
      - ./api_log.log:/app/log/api_log.log
    ports:
      - "8000:8000"
    environment:
      - MEMORY=16G
      - CPUS=8
      - MLFLOW_TRACKING_URI=http://mlflow:5001
    env_file:
      - .env
    networks:
      - mynetwork  
    depends_on:
      - mlflow

  mlflow:
    build:
      context: .  
      dockerfile: Dockerfile_mlflow
    image: mlflow_image:latest  
    container_name: mlflow_docker 
    ports:
      - "5001:5001"
    environment:
      - MLFLOW_BACKEND_URI=/app/MLflow/
    volumes:
      - ./MLFlow:/app/MLflow
    networks:
      - mynetwork  

volumes:
  pg_airflow_data:
  pg_project_data:
  mlflow:

networks:
  mynetwork:
    driver: bridge